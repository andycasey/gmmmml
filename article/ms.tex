
\documentclass{elsarticle}
\journal{Artificial Intelligence Journal}
\usepackage{bm,url,amsmath,amssymb}
%\usepackage{boldsymbol}
\usepackage{color,colortbl} % for \todo command
\usepackage{verbatim} % for \begin{comment}'ing out sections.
\usepackage{algorithm,algorithmicx}
\usepackage{algpseudocode}

% For revision history
\IfFileExists{vc.tex}{\input{vc.tex}}{
    \newcommand{\githash}{UNKNOWN}
    \newcommand{\giturl}{UNKNOWN}}

\newcommand{\todo}[1]{\textcolor{red}{#1}}
\usepackage[T1]{fontenc}
\newcommand{\nbb}[2]{
    % \fbox{\bfseries\sffamily\scriptsize#1}
    \fcolorbox{black}{cyan}{\bfseries\sffamily\scriptsize#1}
    {\sf$\blacktriangleright$\textcolor{blue}{\textit{#2}}$\blacktriangleleft$}
    % \marginpar{\fbox{\bfseries\sffamily#1}}
}

\newcommand{\nbt}[2]{
    % \fbox{\bfseries\sffamily\scriptsize#1}
    \fcolorbox{black}{red}{\bfseries\sffamily\scriptsize#1}
    {\sf$\blacktriangleright$\textcolor{blue}{\textit{#2}}$\blacktriangleleft$}
    % \marginpar{\fbox{\bfseries\sffamily#1}}
}
\newcommand{\nbd}[2]{
    % \fbox{\bfseries\sffamily\scriptsize#1}
    \fcolorbox{black}{green}{\bfseries\sffamily\scriptsize#1}
    {\sf$\blacktriangleright$\textcolor{blue}{\textit{#2}}$\blacktriangleleft$}
    % \marginpar{\fbox{\bfseries\sffamily#1}}
}
\newcommand{\alda}[1]{\nbb{Aldeida}{#1}}


% Define commands
\newcommand{\article}{\emph{Article}}
\newcommand{\acronym}[1]{{\small{#1}}}
\newcommand{\project}[1]{\textsl{#1}}

% Surveys
\newcommand{\apogee}{\acronym{APOGEE}}
\newcommand{\ges}{\acronym{GES}}
\newcommand{\hermes}{\acronym{HERMES}}
\newcommand{\galah}{\acronym{GALAH}}
\newcommand{\fourmost}{\acronym{4MOST}}
\newcommand{\weave}{\acronym{WEAVE}}
\newcommand{\gaia}{\project{Gaia}}
\newcommand{\Cspace}{$\mathcal{C}$-space }
\newcommand{\Cspaceno}{$\mathcal{C}$-space}
% Common terms
\newcommand{\vect}[1]{\boldsymbol{\mathbf{#1}}}

\def\teff{T_{\rm eff}}
\def\logg{\log{g}}

% Have some maaaath.
\def\infinity{\rotatebox{90}{8}}
\def\veccov{\vect{C}}
\def\vecmean{\vect{\mu}}
\def\vectheta{\vect{\theta}}
\def\weight{w}
\def\weights{\vect{\weight}}
\def\datum{y}
\def\data{\vect{\datum}}
\def\likelihood{\mathcal{L}}
\newcommand{\fisher}[1]{\mathcal{F}\left(#1\right)}
\newcommand{\detfisher}[1]{\left|\fisher{#1}\right|}
\newcommand{\prior}[1]{p\left(#1\right)}

% Affiliation(s)
\newcommand{\moca}{
    \affil{School of Physics and Astronomy, Monash University, 
        Melbourne, Clayton VIC 3800, Australia}}
\newcommand{\claytonfit}{
    \affil{Faculty of Information Technology, Monash University,
        Melbourne, Clayton VIC 3800, Australia}}
\newcommand{\caulfieldfit}{
    \affil{Faculty of Information Technology, Monash University,
        Melbourne, Caulfield East VIC 3145, Australia}}



\begin{document}
    \begin{frontmatter}
%       \title{Searching for the Origins of the Galaxy with the Minimum Message Length}
%       \title{Searching for the Origins of the Galaxy with Minimum Message Length mixture modelling}
        \title{Reconstructing the Origins of the Galaxy with Minimum Message Length mixture modelling and new search techniques}
        
%% Group authors per affiliation:
\author[fit]{Aldeida Aleti\corref{mycorrespondingauthor}}
\cortext[mycorrespondingauthor]{Corresponding author}
\ead{aldeida.aleti@monash.edu}
\author[moca,fit]{Andy Casey}
\author[moca]{John Lattanzio}
\author[fit]{David L. Dowe}
\address[moca]{School of Physics and Astronomy, Monash University, Melbourne, Clayton VIC 3800, Australia}
\address[fit]{Faculty of Information Technology, Monash University, Melbourne, Clayton VIC 3800, Australia}

        
        \begin{abstract}
        
        \end{abstract}
        
        
        \begin{keyword}
            Search\sep Minimum Message Length\sep MML\sep mixture models\sep Galactic Archaeology
        \end{keyword}
    \end{frontmatter}
\section{Introduction} 
\label{sec:introduction}

The contributions of this paper are:
\begin{itemize}
\item Solving the galactic archaeology (GA) problem using mixture modelling
% bear in mind or don't forget that GA will mean `genetic algorithms' to many readers
\item Minimum Message Length (MML) formulation
\item 2 search methods
\end{itemize}
\section{Galactic Archaeology}
% "Galaxy produces ‘molecular forests’"
% https://CosmosMagazine.com/space/galaxy-produces-molecular-forests
Galactic Archaeology~\cite{freeman2002new} aims at reconstructing the history of our Galaxy by identifying the remains of a) ancient star formation events and b) smaller galaxies accreted by our own Galaxy during its youth. This is possible because stars form in clusters of 10s to 1000s at a time, all sharing the same composition, being that of the gas cloud from which they formed. This composition is due to the action of previous generations of stars in fusing light elements into heavy ones, and leads an improved understanding of nuclear processes in stars.
%and which stars they operate in.

The idea of Galactic Archaeology relies on identifying stars born together based on their composition. Internal mechanisms change the abundance of the lighter elements, but the heavier ones remain mostly unchanged. These provide a ``chemical tag'' or a ``stellar DNA'' that can be used to identify the stars. 

Observations have shown that each star cluster has a uniform and distinct chemical fingerprint~\cite{pancino2010chemical}. Our aim is to identify stars with the same composition, as these
% were
are widely believed to have been
almost certainly
formed at the same time in the same cluster. We expect this cluster has now dispersed, but the tell-tale chemical tags will be present and will allow us to identify the groups of stars that are the relics of ancient star forming activity.

Little work  has been done to address this problem so far, apart from the work of \cite{mitschang2014quantitative}, who used the \emph{Manhattan distance} metric to  quantify the chemical difference between any two stars, and as a result estimate the level of homogeneity expected within a cluster. This metric calculates the mean weighted absolute difference of abundance levels, $\sigma_C$, between two stars $S^a$ and $S^b$ as follows:
% Given that $\sigma$ is so widely used to mean standard deviation, this notation (to follow) seems a bit unfortunate
$\sigma_C = \sum_{c \in C}w_c|S^a_c-S^b_c|/||C||$, where $C$ is the set of chemical species, $S^a_c$ is the chemical abundance of element $c$ in star $S^a$, $w_c$ is the weight given to species $c$, 
%which is considered 1 for all elements in the current work, 
and $||C||$ is the total number of chemical species. The Manhattan distance values are high for stars from different clusters, and low for stars from the same cluster. Note that this metric is not particularly robust or versatile; for example it is not even invariant to a simple rotation of the co-ordinate axes
% - and nor does it take into account that a difference between two stars of (say) 0.01\% in a rare element might be
% vastly more significant than the same difference in a vastly more abundant element
% Retraction.  It does endeavour to take that into account, via the weights, $w_c$, but not by setting them all to 1.
An important issue is how to set the weights for each chemical species $w_c$, and how they affect 
the results (Mitschang et al.~\cite{mitschang2014quantitative} used $w_c=1$). Another issue is setting the boundary between clusters. It is not clear what a high and low Manhattan distance is; at the moment 
these values are largely subjective. In this paper, we express the problem in terms of mixture modelling and infer the number of clusters, their mixing proportions (or relative abundances) and each cluster's statistical distribution using Minimum Message Length (MML) and two novel search techniques.  

\section{Related Work in Mixture Modelling}

Mixture modelling is composed of three main parts: (i) an estimator of component parameters of a mixture, (ii) an objective functions that scores a mixture, and (iii) a search strategy which identifies the best number of components and their weights. 

Different criteria for estimating component parameters have been used in the literature, with traditional methods using maximum likelihood estimation (MLE)
% Perhaps because ML could refer to many things (e.g., machine learning), maximum likelihood is often (I think usually) called MLE
~\cite{akaike1971determination} or maximum a posteriori probability (MAP)~\cite{shimony1994finding}.
% I re-write the draft below.
% Traditional methods, such as ML or MAP, have not been designed to consider the complexity of the model, but only focus on the goodness-of-fit. They have to be modified with ad-hoc principles in order to ensure that the data is not over-fitted. This is a very important property for astrophysical applications. 
%
% In this work, we focus on the Bayesian Minimum Message Length Principle~\cite{Wallace05}, which unlike MAP estimators, are invariant under non-linear data transformations.
%, and different from MLE, can be used with different probability distributions.  
In this work, we focus on the Bayesian information-theoretic Minimum Message Length (MML) principle~\cite{WallaceBoulton1968, WallaceFreeman1987, WallaceDowe1999a, Wallace05}.
%
The MML principle states that the best explanation of the data is the one that leads to the shortest two-part message~\cite{Wallace05}, where a \textit{message} takes into account both the complexity of the model and its explanatory power (corresponding respectively to the lengths of the first and second parts of the message).
MML has been shown to perform well on a variety of empirical analyses (see, e.g., \cite{viswanathan1999finding,fitzgibbon2004minimum}, with
% further examples and
references to further examples in \cite{Wallace05,dowe2007bayes,Dowe2008a,Dowe2011a}.
Arguments about the statistical consistency
(i.e.,~as the number of data points increases the distributions of the estimates become increasingly concentrated near the true value)
of MML are given in
\cite{DoweWallace1997a,Dowe2011a}.
Unlike the rival Bayesian approach MAP, for continuous parameters MML is a statistically invariant (meaning that $1$-to-$1$ transformations of co-ordinates, such as polar $(r, \theta)$ to Cartesian $(x, y)$, leave the answer unchanged) Bayesian approach~\cite{WallaceFreeman1987,WallaceDowe1999a,Wallace05,dowe2007bayes}. It allows the modelling of prior beliefs in the calculation of the best model parameters. If one has a particular reason to expect one value rather than another, a prior probability can be chosen to represent that belief. Of course, we can also model the data as objectively as Bayesianism permits. 

Two other well-known methods based on information theory are the Akaike Information Criterion (AIC)~\cite{akaike1974} and Bayesian Information Criterion
(BIC)~\cite{schwarz1978estimating}, both of which endeavour to temper Maximum Likelihood's tendency to over-fit and under-estimate the noise.
% Unlike Maximum Likelihood, MML takes into account model complexity - and, unlike MAP, it is invariant under non-linear continuous transformations.
In MML terms, Maximum Likelihood can be thought of as MML without paying for the first part of the message.

AIC~\cite{akaike1974} chooses a model 
% that minimizes
with the motivation of minimizing
the Kullback-Leibler
% distance
divergence
between
% KL divergence is generally asymmetric
the model and the truth, and is defined as:

\begin{equation}
AIC = -2 (\ln ( f(\vec{x} | \vec{\theta} )) + 2 k
\end{equation}
where the likelihood $f(\vec{x} | \vec{\theta})$ is the probability of the data $\vec{x}$ given a model parameterised by $\vec{\theta}$,
and $k$ is the number of free parameters in the model. 

BIC~\cite{schwarz1978estimating} uses a constant penalty equal to $(\log N)/2$ for each free parameter in the model,
where $N$ is the number of data points.
Minimum Description Length (MDL)~\cite{rissanen1978modeling} was independently introduced in the same year (1978) as BIC with a very similar formula, a decade after MML \cite{WallaceBoulton1968}.
Both AIC and BIC/MDL associate parameter costs with the number of free parameters and not their values. In this case, all models of a particular probability distribution have the same cost, regardless of the respective means, covariance matrices and Fisher information,
which may hinder accurate inference.   

% Comparisons of MML to other methods
% clearly indicate MML's theoretical and empirical superiority.
% Let's not overly annoy the reviewers and readers.
Examples of MML's promise against rival methods include comparisons with AIC~\cite{dowe2007bayes}, BIC \cite{agusta2002mml}, MDL~\cite{viswanathan1999finding} and MLE~\cite{dowe2007bayes} - and also in statistical econometric time series autoregression \cite{fitzgibbon2004minimum}, machine learning problems \cite{viswanathan1999finding},
angular data \cite{WallaceDowe1993,DoweOliverWallace1996}
and using this modelling \cite{WallaceDowe1993} to infer the order in which various parts of a protein fold~\cite{edgoose1998mml}. 
%
A further example is the problem of deciding whether or not some 1-dimensional data generated uniformly in the interval $[ 0, 1 ]$ has a gap in it. Curiously, even when there is clearly no gap, AIC infers that there is a gap, whereas MML gives the right answer \cite{dowe2007bayes}. 

Very importantly, the MML message structure enables us to robustly deal with missing data \cite{WallaceDowe1994b,WallaceDowe1997,WallaceDowe2000}. Other methods often replace missing values with the mean - in turn, reducing the true standard deviation - or various other imputed methods. In MML, on the other hand, missing data can easily be left out from the calculations from modelling done so far, simply encoding the missing data as missing \cite{WallaceDowe1994b,WallaceDowe1997a,WallaceDowe2000}, which is very useful when modelling astrophysical data, where missing observations are quite common.

The parameters of a mixture model can be estimated using Expectation-Maximisation (EM)~\cite{dempster1977maximum}. The EM algorithm has been used with MML to infer Gaussian mixtures under different simplifying assumptions, such as assuming that the covariance matrices are diagonal~\cite{WallaceDowe1994b, WallaceDowe1997a, WallaceDowe2000} (where it is sometimes called an `adjust cycle') and \cite{oliver1996unsupervised}, or approximating the probabilities of mixture parameters~\cite{figueiredo2002unsupervised,roberts1998bayesian}. 

The search algorithm employed in the majority of these approaches is based on running EM iteratively for different numbers of components~\cite{WallaceDowe1994b, WallaceDowe1997a, WallaceDowe2000, oliver1996unsupervised,roberts1998bayesian,biernacki2000assessing}. The search method proposed by Figueiredo and Jain~\cite{figueiredo2002unsupervised}, on the other hand, starts with a large number of components, and iteratively deletes components. An extension of this method was proposed by Kasarapu and Allison~\cite{kasarapu2015minimum}, which starts with one components, and selectively splits, deletes, or merges components in an attempt to minimise the message length. While this method removes the overhead introduces by the search algorithm of Figueiredo and Jain~\cite{figueiredo2002unsupervised}, the search remains greedy, and can be prone to premature convergence in suboptimal solutions. The search methods introduced in this paper, incorporate a perturbation step which encourages explorations of new local optima.

{\color{blue}References that we/you seem to be missing include
\cite{WallaceBoulton1968, WallaceFreeman1987, WallaceDowe1999a}
% these three above are probably the three most-cited MML papers
and
WallaceDowe1993,WallaceDowe1994b,DoweOliverWallace1996,
% these three works above are the earliest MML works on angular data and mixture modelling with angular data
DoweWallace1997
% the above is a work about MML statistical consistency
WallaceDowe1997,WallaceDowe2000
% these two papers above are MML mixture modelling papers
Dowe2008a,Dowe2011a
% these two papers above are MML survey pieces

Things that could also be added include
ConwaySloane1988
% the above is a book on lattices and lattice constants
EdwardsDowe1998
% this was (and still is) a work on single latent factors in mixture modelling with application to astro' data
McLachlanPeel2000
% this is quite a popular book on finite mixture models
}
\section{Minimum Message Length (MML)}
A complex model is not an optimal one, unless its complexity is justifiable by the added explanatory power. There is an established connection between simplicity and truth, as beautifully elucidated by the Occam's Razor principle. MML is a statistical technique that formalizes and quantifies this principle.

The basic idea is to consider the transmission of a set of data and a model that describes it. Clearly one can make increasingly complex models to obtain better fits -- think of epicycles in the solar system. Suppose we try to best represent N data points. 

Traditional curve fitting methods would give a polynomial of degree N-1, which is perhaps the most precise, but it is also the most complex, and the one that most models the noise, by spuriously {\it over-fitting non-existing patterns.\/}

Over-fitting can result in a model that may be far from the truth. MML will only consider a more complex model if the encoding of the complex model and the data is more efficient  than the less complex model, i.e. it finds the minimum message length needed to transmit the data and the model. It is this \textit{true} model that MML aims to find.

The MML principle states that the best explanation of the data is the one that leads to the shortest message~\cite{Wallace05}, 
where a \textit{message} takes into account both the complexity of the model, and its explanatory power. 

In essence, MML infers the most concise way of encoding the model, $\theta$, and the data, $x$, given the model, $x|\theta$. A complex model will produce a longer first part of the message than a simple model, since more quantities must be stated. On the other hand, the length of the second part of the message decreases with more accurate and complex models, since less data has to be described fully.

The \textit{message} must encode two parts: the model, and the data. The encoding of the message is based on Shannon's information theory. According to Shannon, the information we gain from an event $e$ is $h(e) = - \log_2 p(e)$, where $p(e)$ is the probability of that event. The information content is biggest for the improbable outcomes, and smallest for outcomes that we are almost certain about. An outcome that has a probability close to one has close to zero information content, since we don't learn anything new from it, whereas the rarer events have much higher information content. 

%%%% This section below looks like a variant of a lift-out from the ARC DP16 grant proposal.
%%%% One problem with the presented text is that 4 does not equal 6.
%%%% Mightn't it be easier to talk about Morse Code?
%%%% In certain parts of the astronomy literature, such an example (when tidied up) might be suitable, but I think that - in the AI Journal - such an example needs to be no longer than short. I think that, here, all we need to do is mention Morse code and (e.g.) the Huffman code example from \cite[sec. 2.1.4]{Wallace05}.
% Consider the simplified illustrative example of rolling an unbiased four-sided die (d4), where each of the four outcomes $(1,2,3,4,5,6)$ have equal probability $p_i=0.25$. The information content in each datum is $- \log_2 0.25 = 2$. In total $\sum_{i=1}^4-\log_2 (p_i) = 2 + 2 + 2 + 2 = 8$ bits are required to describe the four outcomes. The model description could be $\{00,01,10,11\}$:~this is the first part of the message. 

% Now assume that the die is biased, with probabilities $p(1)=0.5, p(2)=0.25, p(3)=0.125$, and $p(4)=0.125$. In this case, the number of bits required to represent each alternative is different. One bit is required for 1, since $-\log_2 0.5 = 1$, and the codeword could be the single binary value 1. Similarly, 2 bits are required for the outcome 2, which can be represented with codeword $01$. It follows that possible codewords for the remaining alternatives, all using the bit ``1'' to indicate the end of each datum, are $001$ for 3, $0001$ for 4, $00001$ for 5, and $000001$ for six. 

% The length of the model required to describe the outcomes of the biased die would be $\sum_{i=1}^4-\log_2 (p_i) = 1 + 2 + 3 + 3 = 9$ bits, and the codewords would be $\{0,10,110,111\}$. The second description is 1 bit longer than the first, but, since the most frequent events have a shorter codeword, the second part of the message (the data given the model) is shorter. The overall message length of the second description is shorter than the first, when applied to the biased die. 

% To illustrate this point let's encode 1000 outcomes of a biased die, where 500 outcomes are 1, 250 are 2, 125 are 3, and 125 are 4. With the first model each outcome is encoded with two bits, so the length of the second part of the message is $1000\times 2=2000$. The total message length is the sum of the model length and the length of the data given the model, or 2008 bits. But if we encode the data with the second model, the length of the data given the model is $500+(250\times2)+(125\times3)+(125\times3)=1750$. The total length is $1750+9=1759$. Thus the second model, although it is more complex (takes more bits to describe), creates a shorter encoded message, 
% and hence is a better description than the shorter model.
%%%% As per above comment, I think that the text from the above comment to here can all go.

Formally, the length of the first part of the message is $- \log_2 h(\vec{\theta})$, and that of the second part is $- \log_2 f(\vec{x}|\vec{\theta})$, where $h(\theta)$ is the prior probability of model $\theta$, and $f(x|\theta)$ is the likelihood of data $x$ assuming a model parameterised by $\vec{\theta}$. The total message length is the sum of the two parts, i.e. $- \log_2 h(\vec{\theta}) - \log_2f(\vec{x} | \vec{\theta})$, which is minimised precisely when $h(\vec{\theta}) f(\vec{x}| \vec{\theta})$ is maximised.
The multiplication of the prior and the likelihood is proportional to the posterior {\em probability} $g(\theta|x)$. An important but often misunderstood subtle point is that we are quantising (or rounding off in) parameter space, and so $h(\theta)$ corresponds to a {\em probability} and {\em not} a density. Thus, the MML estimate can in some sense be thought of as maximising $h(\theta)f(x|\theta)$ and in turn (in some sense) equal to the posterior mode. This is similar in spirit to the (Bayesian) Maximum A Posteriori (MAP) posterior mode, but
(partly because both terms $- \log_2h(\vec{\theta})$ and $\log_2 f(\vec{x}|\vec{\theta})$ here correspond to probabilities)
with the benefit of statistical invariance under re-parameterisation
- for further discussion, see, e.g., ~\cite{WallaceFreeman1987,WallaceDowe1999a,Wallace05,dowe2007bayes} or immediately below.


\section{MML for Mixture Modelling}
MML is used to identify groups (also known as ``classes'') of stars with the  same composition. Mixture modelling enables the partitioning of the data into overlapping groups. 

In MML we model this by constructing a two-part message, with the first part describing the mixture model (or hypothesis, $\theta$) and the second part representing the encoded data ($x$) given the hypothesis. The MML \textit{message} contains the following components: 

\begin{itemize}
\item[\textbf{1a)}] A statement of the number of groups (or classes, or Gaussian mixture components), $K$. Each class is assumed to have the same prior probability, equal to $2^{-K}$. The length of this part is equal to the negative logarithm of the prior, i.e. $K$. 
\item[\textbf{1b)}] The relative abundances (or relative weights or mixing proportions) of the amount(s) of data in each group.
Only $(K - 1)$ weights need be encoded because $\sum_{k=1}^{K}\weight_k = 1$.
The length of this part is equal (within a small constant) to the negative log of the multiplication of the prior probabilities of each group ($h(\vec{p})$) divided by the square root of the expected
Fisher information ($\sqrt{F(\vec{p_k})}$). 
        The Fisher information matrix contains the expected second-order partial derivatives of the log-likelihood function, and quantifies how sharply the likelihood function peaks, and as a result how precisely the parameters should be stated.
    
    In the case of uniform prior probabilities $h(\vec{p}) = (K-1)!$, and the Fisher information $F(\vec{p}) = N^{K-1} / (\prod_{k=1}^{K} p_k)$. It follows that the length of this part is equal to $- \log( h(\vec{p}) \times (1 / \sqrt{{\kappa_K}^K F(\vec{p})}) ) = - \log((K-1)!) + (K/2) \log(\kappa_K) + ((K-1)/2) \log(N) - (1/2) \sum_{k=1}^{K} \log(p_k)$, where $\kappa_K$ is a lattice constant (varying between $1/12$ and $1/(2 \pi e)$).

\item[\textbf{1c)}]  The component parameters $\vecmean$ and $\veccov$ for all $K$ Gaussian components.

\item[\textbf{2)}] The data given the model and its parameters.
% , whose cost is the well-known statistical log-likelihood. In the case of a Normal distribution, this % would appear to a first approximation to equal 
%   $$-\log (p_k(x) ~ \frac{1}{\sqrt{2 \pi} \sigma_k} ~ e^{-\frac{(x-\mu_k)^2}{2\sigma_k^2}}).$$
To a first approximation, this would appear to be
$ \min_{k} -\log (p_k(x) ~ \frac{1}{\sqrt{2 \pi} \sigma_k} ~ e^{-\frac{(x-\mu_k)^2}{2\sigma_k^2}}) \rm{,}$
minimising (over cluster indices $k$) $ -\log (p_k(x)) $ (which is the cost of encoding the particular cluster, $k$) plus the negative log-likelihood given cluster $k$'s parameters.
However, as per \cite{WallaceDowe1994b,WallaceDowe1997a,WallaceDowe2000}\cite[sec. 6.8]{Wallace05}, the cost can be reduced to
$ -\log ( \Sigma_{k=1}^{K} p_k(x) ~ \frac{1}{\sqrt{2 \pi} \sigma_k} ~ e^{-\frac{(x-\mu_k)^2}{2\sigma_k^2}}) $
\end{itemize}

Parts 1a, 1b and 1c encode the parameters from the clustering process, and part 2 encodes the likelihood (the data given the hypothesis), which represents the goodness-of-fit. 

The estimation of the multinomial parameters from part 1b is done by quantifying the trade-off between stating the parameters imprecisely, so as to have higher prior probability in the relevant uncertainty region and keep the first part of the message short, and the goodness of fit to the data, which is typically better if we state the parameter estimates more precisely. 

Quantitatively, this is done by minimising the sum of the lengths of the three components (1a, 1b, 1c and 2 above). If MML finds that the data for a specific element do not affect the selection of groups, then it will {\it ignore that element.\/} This could tell us something important - such as that there is a large random component for that element or that there are significant observational errors ruining the usefulness of that measurement. These are significant insights into the data that are, importantly, quantified by the MML method.


\subsection{Message Length}


The MML principle requires that we encode everything required to reconstruct the message, including our prior beliefs on the model parameters. Consider that we have $N$ data points each with $D$ dimensions which are to be modelled by a finite number of Gaussian mixtures. Therefore the message for a finite number of Gaussian mixtures must encode:

% \begin{enumerate}
% k has been used previously in describing AIC and BIC.
% If k is the number of parameters, then it will be neither (the number of clusters) nor an index running from 1 to (the number of clusters).
% \item
% \begin{itemize}
% \item (1a) ~ The number of Gaussian mixture components, $K$.
% \item (1b) ~ The relative weights $\weights$ of the $K - 1$ Gaussian components. Only
%       $K - 1$ weights need be encoded because $\sum_{k=1}^{K}\weight_k = 1$.
% \item (1c) ~ The component parameters $\vecmean$ and $\veccov$ for all $K$ Gaussian
%       components.
% \end{itemize}
% \item (2) ~ The data, given the model parameters.
%  \item The lattice quantisation constant $\kappa(Q)$ for the number of model       parameters $Q$.
% \end{enumerate}

We provide the message length of each component in turn before outlining our
full objective function. We assume a prior on the number of mixtures $K$ of
% $\prior{K} \propto 2^{-K}$
$\prior{K} = 2^{-K}$
\cite[p279, sec. 6.8.2, (a)]{Wallace05} such that the length of the 
optimal lossless message to encode $K$ is 
\begin{equation}
    I(K) = -\log{\prior{K}} = K\log{2} + \textrm{constant}.
\end{equation}

We assume a uniform prior on the individual weights $\weight_{k}$. The weights
$\weights$ are drawn from a multinomial distribution such that they can be 
optimally encoded in a message with length~\todo{\cite{someone}}:
\begin{equation}
  I(\weights) 
    = \frac{K - 1}{2}\log{N} 
    - \frac{1}{2}\sum_{k=1}^{K}\log\weight_k 
    - \log{\Gamma{\left(K\right)}} \quad .
\end{equation}


In order to properly encode the means $\vecmean$ and covariance matrices
$\veccov$ for all $K$ mixtures, we must encode both our prior belief on 
those parameters and the determinant of the expected Fisher information 
matrix. For the $k$-th mixture the joint message length is
\begin{eqnarray}
  I(\vecmean_k,\veccov_k) &=& -\log{\left(\frac{\prior{{\vecmean_k,\veccov_k}}}{\sqrt{\detfisher{{\vecmean_k,\veccov_k}}}}\right)} \nonumber \\ 
  I(\vecmean_k,\veccov_k) &=& -\log{\prior{{\vecmean_k,\veccov_k}}} + \frac{1}{2}\log{\detfisher{{\vecmean_k,\veccov_k}}}
\end{eqnarray}

\noindent{}and for all mixtures:
\begin{equation}
  I(\vecmean,\veccov) = -\sum_{k=1}^{K}\log{\prior{{\vecmean_k,\veccov_k}}} + \frac{1}{2}\sum_{k=1}^{K}\log{\detfisher{{\vecmean_k,\veccov_k}}} \quad .
  \label{eq:I_component_params}
\end{equation}

We use an improper uniform prior on the mean $\vecmean$ for all mixtures such
that 
    $\mathcal{U}(\vecmean) = (-\infty, +\infty)$.
However,  the MML principle \todo{demands} proper priors. In practice we make 
this prior proper by assuming the bounds on $\prior{\vecmean}$ are very large, but 
we do so without actually specifying the value of the bounds because they add 
only a constant value to the total message length\todo{\cite{someone}}.
We adopt a conjugate inverted Wishart prior for the covariance matrix of an
individual mixture
- e.g.,
%[]
\cite[Section 5.2.3]{Schafer_1997}:


\begin{equation}
  \prior{{\vecmean_k, \veccov_k}} \propto |\veccov_k|^{\frac{D+1}{2}} \quad .
  \label{eq:covariance-prior}
\end{equation}


% Casey: Reword following paragraph because it is clumsy.
Following Eq. \ref{eq:I_component_params}, we require the logarithm of the
determinant of the expected Fisher information matrix for $\vecmean_k$ and
$\veccov_k$ in order to encode the full message length. We approximate the
determinant of the Fisher information $\detfisher{{\vecmean_k, \veccov_k}}$ 
as the product of $\detfisher{\vecmean_k}$ and $\detfisher{\veccov_k}$ 
\cite{Oliver_1996,Roberts_1998}. In order to derive $\detfisher{\vecmean_k}$
we take the second derivative of the log-likelihood function 
$\likelihood(\data|\vecmean,\veccov)$, which yields:
\begin{equation}
  \detfisher{\vecmean_k} = (N\weight_k)^{D}|\veccov_k|^{-1} \quad .
\end{equation}
We make use of an analytical expression for $\detfisher{\veccov_k}$
\cite{Dwyer_1967,Magnus_1988}:
\begin{equation}
  \detfisher{\veccov_k} = (N\weight_k)^\frac{D(D+1)}{2}2^{-D}|\veccov_k|^{-(D+1)} \quad .
\end{equation}
Combining these expressions gives our approximation for the determinant of
the expected Fisher information $\detfisher{{\vecmean_k,\veccov_k}}$:
\begin{eqnarray}
  \detfisher{{\vecmean_k,\veccov_k}} & \approx & \detfisher{\vecmean_k} \cdot \detfisher{\veccov_k} \nonumber \\
  \detfisher{{\vecmean_k,\veccov_k}} & \approx & (N\weight_k)^{D}|\veccov_k|^{-1}(N\weight_k)^\frac{D(D+1)}{2}2^{-D}|\veccov_k|^{-(D+1)} \nonumber \\
  \detfisher{{\vecmean_k,\veccov_k}} & \approx & (N\weight_k)^\frac{D(D+3)}{2}2^{-D}|\veccov_k|^{-(D+2)} \quad .
  \label{eq:mixture-component-det-fisher}
\end{eqnarray}


Substituting Eqs. \ref{eq:covariance-prior} and \ref{eq:mixture-component-det-fisher} 
into Eq. \ref{eq:I_component_params} gives the message length to encode the component
parameters $\vecmean$ and $\veccov$ for all $K$ mixtures:

\begin{eqnarray}
  I(\vecmean,\veccov) &=& -\sum_{k=1}^{K}\log{\prior{{\vecmean_k,\veccov_k}}} + \frac{1}{2}\sum_{k=1}^{K}\log{\detfisher{{\vecmean_k,\veccov_k}}} \nonumber \\
  I(\vecmean,\veccov) &=& -\sum_{k=1}^{K}\log{|\veccov_k|^{\frac{D + 1}{2}}}
    + \frac{1}{2}\sum_{k=1}^{K}\log{\left[(N\weight_k)^\frac{D(D+3)}{2}2^{-D}|\veccov_k|^{-(D+2)}\right]} \nonumber \\
  I(\vecmean,\veccov) &=& \frac{D(D+3)}{4}\sum_{k=1}^{K}\log{N\weight_k} -\frac{(D + 1)}{2}\sum_{k=1}^{K}\log{|\veccov_k|}
    -\frac{KD}{2}\log{2} \nonumber \\
  && -\frac{D+2}{2}\sum_{k=1}^{K}\log{|\veccov_k|} \nonumber \\
  I(\vecmean,\veccov) &=& \frac{D(D+3)}{4}\sum_{k=1}^{K}\log{N\weight_k} -\frac{2D+3}{2}\sum_{k=1}^{K}\log{|\veccov_k|} - \frac{KD}{2}\log{2} 
\end{eqnarray}


If we assume that the data have homoskedastic noise properties, then the 
precision of each measurement $\mathcal{E}$ relates the probability of a
datum $p(\datum_n)$ can be related to the probability density of
the datum, given the model 
$p(\datum_n) = \mathcal{E}^{D}p(y_i|\mathcal{M})$.
In this work we adopt $\mathcal{E} = 0.01$, but we note that the value of
$\mathcal{E}$ has no effect on our inferences.  The message length of encoding
a datum given the model is,
\begin{equation}
  I(\data_n) = -\log{p(\data_n)} = -D\log\mathcal{E} - \log\sum_{k=1}^{K}\weight_{k}f_{k}(\data_n|\vecmean_k,\veccov_k)
\end{equation}

\noindent{}and for the entire data:
\begin{eqnarray}
  I(\data|\vectheta) &=& -ND\log\mathcal{E} - \sum_{n=1}^{N}\log\sum_{k=1}^{K}w_{k}f_k(\data_n|\vecmean_k,\veccov_k) \quad .
\end{eqnarray}

The final part of the message is to encode the lattice quantisation constant,
which arises from the approximation to the (so-called) strict MML, where
parameters are quantised into intervals in high dimensional space in order to
be losslessly encoded.  We use an approximation of the lattice constant
\citep[see Sections 5.1.12 and 3.3.4 of ]
%[]
{Wallace_2005} such that,
\begin{equation}
  \log\kappa(Q) = \frac{\log{Q\pi}}{Q} - \log{2\pi} - 1 \quad ,
\end{equation}

\noindent{}where $Q$ is the total number of free parameters in the model. Each
Gaussian component in the mixture requires $D$ parameters to specify $\vecmean$,
and a covariance matrix with off-diagonal terms requires $\frac{1}{2}D(D+1)$ 
parameters. Because only $K - 1$ weights need to be encoded, the total number of free
parameters is
\begin{equation}
    Q = K\left[\frac{D(D+3)}{2} + 1\right] - 1 \quad .
    \label{eq:number-of-parameters}
\end{equation}



%  Here

 Each $k$-th mixture is a $D$-dimensional Gaussian that contributes a relative weight $\weight_k$ such that $\sum_{k=1}^{K}\weight_k = 1$. We will assume that the data have homoscedastic noise properties.

The data have the same error value, $y_{err}$, in all $D$ dimensions, for all $N$ observations.

The message length for the data is given by:

Here we introduce the individual components that contribute to the message length.

Combining these expressions, the complete message length for $K$ mixtures is given by

\begin{eqnarray}
I_K & = & K\log{2} % I_k
    + \frac{(K - 1)}{2}\log{N} - \frac{1}{2}\sum_{k=1}^{K}\log{w_k} - \log{|\Gamma(K)|} % I_w
    +\mathcal{L}(\data|\vectheta) \nonumber \\
    & -& DN\log{y_{err}} % Likelihood
   +  \frac{1}{2}\sum_{k=1}^{K}\left[\frac{D(D+3)}{2}\log{{Nw_k}} - (D + 2)\log{|\veccov_k|} - D\log{2}\right]\nonumber \\% I_t
    &-& \frac{Q}{2}\log(2\pi) + \frac{\log{Q\pi}}{2} % lattice, minus the lattice of part 2
\end{eqnarray}

\noindent{}where $Q = \frac{1}{2}DK(D + 3) + K - 1$, the number of free parameters, and
$\mathcal{L}$ is the log-likelihood of a multivariate Gaussian distribution.

\section{Search Methods}

\subsection{The MML$_{K+D}$ Method}
Say we wanted to calculate whether another mixture was warranted. If another
mixture were preferred then we would want:

\begin{eqnarray}
  \Delta{}I_{K+1} - I_{K} < 0
\end{eqnarray}

The expression is given as:
\begin{eqnarray*}
\Delta{I_{K+1} - I_K} & = & (K + 1)\log{2} - K\log{2} \\ % I_k^(new) - %I_k^(old)
  &&+ \frac{(K)}{2}\log{N} - \frac{1}{2}\sum_{k=1}^{K+1}\log{w_k}^{(new)} - \log{|\Gamma(K+1)|} \\ % I_w^(new)
  &&- \frac{(K - 1)}{2}\log{N} + \frac{1}{2}\sum_{k=1}^{K}\log{w_k} + \log{|\Gamma(K)|}\\ % I_w^(old)
  &&+ \mathcal{L}^{(new)} - DN\log{y_{err}} \\ % Likelihood (new)\\
  &&- \mathcal{L}^{(old)} + DN\log{y_{err}} \\ % Likelihood (old) \\
  &&+ \frac{1}{2}\sum_{k=1}^{K+1(new)}\left[\frac{D(D+3)}{2}\log{{Nw_k}} - (D + 2)\log{|\veccov_k|} - D\log{2}\right] \\ % I_t^(new)
  &&- \frac{1}{2}\sum_{k=1}^{K(old)}\left[\frac{D(D+3)}{2}\log{{Nw_k}} - (D + 2)\log{|\veccov_k|} - D\log{2}\right] \\ % I_t^(old)
  &&- \frac{Q^{(new)}}{2}\log(2\pi) + \frac{\log{Q^{(new)}\pi}}{2} \\ % lattice, minus the lattice of part 2
  &&+ \frac{Q^{(old)}}{2}\log(2\pi) - \frac{\log{Q^{(old)}\pi}}{2} % lattice, minus the lattice of part 2
\end{eqnarray*}

\noindent{}By making use of $\log{\Gamma(K)} - \log{\Gamma(K + 1)} = -\log{K}$ and re-arranging the expression:

\begin{eqnarray}
\Delta{}I_{K+1} - I_K &=& \log{2} % \Delta{}I_k
    + \frac{1}{2}\log{N} - \log{K} - \frac{1}{2}\left(\sum_{k=1}^{K+1}\log{w_k^{(new)}} - \sum_{k=1}^{K}\log{w_k^{(old)}}\right) \nonumber \\ % \Delta{}I_w
& +& \mathcal{L}^{(new)} - \mathcal{L}^{(old)} \nonumber \\ % likelihood
& +& \frac{1}{2}\left[\frac{D(D+3)}{2}\left(\sum_{k=1}^{K+1}\log{Nw_k^{(new)} - \sum_{k=1}^{K+1}\log{Nw_k^{(old)}}} \right) \right.\nonumber\\
&-& \left.\left(D+2\right)\left(\sum_{k=1}^{K+1}\log{|\veccov_k|^{(new)}} - \sum_{k=1}^{K+1}\log{|\veccov_k|^{(old)}}\right)\right] \nonumber \\
& +& \frac{\log(2\pi)}{2}(Q^{(old)} - Q^{(new)}) + \frac{\pi}{2}\left(\log{Q^{(new)}} - \log{Q^{(old)}}\right)
\label{eq:13}
\end{eqnarray}

Expanding the $Q$ terms:

\begin{eqnarray}
Q^{(old)} - Q^{(new)} &=& \frac{1}{2}DK(D + 3) + K - 1 - \frac{1}{2}D(K + 1)(D + 3) + (K + 1) - 1 \nonumber \\
Q^{(old)} - Q^{(new)} &=& -\frac{1}{2}D(D+3) + 2K  - 1
\label{eq:14}
\end{eqnarray}

\noindent{}And making use of the following logarithmic identities,
\begin{eqnarray}
  \log{Q^{(new)}} &=& \log{\left(\frac{1}{2}D(K+1)(D + 3) + K\right)} \nonumber \\
                  &=& \log{\left(\frac{1}{2}D(K+1)(D + 3)\right)} + \log{\left(1 + \frac{K}{\frac{1}{2}D(K+1)(D + 3)}\right)} \\
  \log{Q^{(old)}} &=& \log{\left(\frac{1}{2}DK(D + 3) + K - 1\right)} \nonumber \\
                  &=& \log{\left(\frac{1}{2}DK(D + 3)\right)} + \log{\left(1 + \frac{K - 1}{\frac{1}{2}DK(D + 3)}\right)}
\end{eqnarray}


\noindent{}gives us,

\begin{eqnarray}
  \log{Q^{(new)}} &- \log{Q^{(old)}} = \log{\left(\frac{1}{2}D(K+1)(D + 3)\right)} - \log{\left(\frac{1}{2}DK(D + 3)\right)} \nonumber \\
                                    &+ \log{\left(1 + \frac{K}{\frac{1}{2}D(K+1)(D + 3)}\right)} - \log{\left(1 + \frac{K - 1}{\frac{1}{2}DK(D + 3)}\right)}.
\end{eqnarray}
The second row of terms can be ignored because they are very small (typically less than 1 bit). This is because as $K \rightarrow \infinity$, $2K/D(K+1)(D+3) \rightarrow 1$, thus $\log{\left(1 + \frac{K}{\frac{1}{2}D(K+1)(D + 3)}\right)} \rightarrow \log{2}$. Similarly as $D \rightarrow \infinity$, $2K/D(K+1)(D+3) \rightarrow 0$.

As $K \rightarrow \infinity$ then $2(K-1)/DK(D+3) \rightarrow 1$ and as $D \rightarrow \infinity$ then $2(K-1)/DK(D+3) \rightarrow 0$ and thus $\log{\left(1 + \frac{K - 1}{\frac{1}{2}DK(D + 3)}\right)} \approx 0$.

\noindent{}Ignoring these minor terms:

\begin{eqnarray}
  \log{Q^{(new)}} - \log{Q^{(old)}} &\approx& \log{\left(\frac{1}{2}D(K+1)(D + 3)\right)} - \log{\left(\frac{1}{2}DK(D + 3)\right)} \nonumber \\
  \log{Q^{(new)}} - \log{Q^{(old)}} &\approx& \log{(K + 1)} - \log{K}
  \label{eq:19}
\end{eqnarray}

\noindent{}Substituting Eqs. \ref{eq:19} and \ref{eq:14} into \ref{eq:13} yields:

\begin{eqnarray}
\Delta{}I_{K+1} &-& I_K \approx \log{2} % \Delta{}I_k
    + \frac{1}{2}\log{N} - \log{K} - \frac{1}{2}\left(\sum_{k=1}^{K+1}\log{w_k^{(new)}} - \sum_{k=1}^{K}\log{w_k^{(old)}}\right) \nonumber \\ % \Delta{}I_w
& +& \mathcal{L}^{(new)} - \mathcal{L}^{(old)} \nonumber \\ % likelihood
& +& \frac{1}{2}\left[\frac{D(D+3)}{2}\left(\sum_{k=1}^{K+1}\log{Nw_k^{(new)} - \sum_{k=1}^{K+1}\log{Nw_k^{(old)}}} \right) \right.\nonumber \\
&-& \left.\left(D+2\right)\left(\sum_{k=1}^{K+1}\log{|\veccov_k|^{(new)}} - \sum_{k=1}^{K+1}\log{|\veccov_k|^{(old)}}\right)\right] \nonumber \\
& +& \frac{\log(2\pi)}{2}(-\frac{1}{2}D(D+3) + 2K  - 1) + \frac{\pi}{2}\left(\log{(K + 1)} - \log{K}\right)
\end{eqnarray}

\begin{algorithm}[H]
\caption{The MML$_{K+D}$ Method.}
\label{alg:MMLKD}
\begin{algorithmic}[1]
\State {\bf \sc input:} $K$ \Comment{Number of initial components.}
\State current $\leftarrow$ $K$ component mixture
\While{TRUE}
\EndWhile
\State {\bf \sc output:} 
\end{algorithmic}
\end{algorithm}

\subsection{Method of Moments}
% \alda{This is a copy paste from David's email with minor changes}
%
% Re the case of two (or more) classes with the same mean:
% In 2D, a simple case in point is two classes with the same mean
% such that $\sigma_{1, x} = \sigma_{2, y} >> \sigma_{2, x} = \sigma_{1, y}$.
% Andy said that this and higher dimensional stuff can be picked up
% by dropping to 1 dimension along the eigenvector of largest eigenvalue.
%
%
% I wonder also about the merits of dropping to 2 dimensions (if we have at least 2 attributes), being the eigenvectors of the two largest eigenvalues. 
%
% One could possibly check whether uniform radially directly from the original 2D projection without having to project it to a circle (i.e., it might be pretty easy to work out what the angle of each point would be post-transformation without having to do the transformation).  The 2D projection is 1D in the sense that we only care about the angle around the circle.  And then some test (e.g., Kolmogorov-Smirnov test) for whether data is uniform in radial distribution.  If not uniform, then perhaps grounds for splitting.
%
% Re looking at things in 1 dimension (not 2D followed by 1D [0, 2 pi) angle
% as above but rather projecting on eigenvector of largest eigenvalue), we
% discussed moments - e.g., \url{https://en.wikipedia.org/wiki/Normal_distribution}.
% So, 2nd, 4th, 6th and 8th moments of a true Gaussian are 1, 3, 15, 105.
%

We informally outline here the method of moments form of search heuristic and then look at how it might be applied.

Let's suppose we have a 1D component.  We'll shift it (without loss of generality, w.l.o.g.) so that its mean is 0 and we'll shrink or expand it (again, w.l.o.g.) so that its s.d. ($\sigma$) is 1 (and variance v is 1).
We consider now splitting it into K components with mean 0,
mixing weights (or proportions) $w_1, ..., w_i, ..., w_K$ and s.d.s $\sigma_i$ (and variances $v_i = (\sigma_i)^2$).
Let's set $K = 2$, so $w_2 = 1 - w_1$.

Noting that the 2nd, 4th, 6th and 8th moments of a true Gaussian distribution are 1, 3, 15, 105,
suppose the moments of our current component are 1, $3 \alpha_4, 15 \alpha_6$ and $105 \alpha_8, ... $.
We get some equations - if we have the correct number of equations,
they're simultaneous equations.

If we get too many equations (by using too many moments), then it
becomes something of a fitting or regression problem to choose
$w_1, ..., w_i, ..., w_K$ and s.d.s $\sigma_i$, variances $v_i = (\sigma_i)^2$.

Our simultaneous equations are:

\begin{align*}
w_1 v_1       + (1 - w_1) v_2       &=    1\\
w_1 (v_1)^2  + (1 - w_1) (v_2)^2  &=    3 \alpha_4\\
w_1 (v_1)^3  + (1 - w_1) (v_2)^3  &=   15 \alpha_6\\
w_1 (v_1)^4  + (1 - w_1) (v_2)^4  &=  105 \alpha_8
\end{align*}

% These might be messy to solve on the fly  but one could pre-process by solving many of these beforehand (actually, approximately solving these beforehand, as it's only
% a heuristic, anyway) at the very start of the program while loading in the data.

So, for $1000 \times 1000 = 10^6$ different values of $\alpha_4$ and $\alpha_6$
or for $100 \times 100 \times 100 = 10^6$ different values of $\alpha_4, \alpha_6$ and $\alpha_8$
one could pre-compute {\em approximate} 1,000,000ish `solutions' to the above simultaneous equations.

Then, when running the program with real actual values of $\alpha_4$ and $\alpha_6$ (and $\alpha_8$),
one could just grab something from nearby in this pre-computed look-up table
as putative values for $w_1, v_1, v_2$ (and etc., if need be).

% As an example of this algorithm, consider the mixture $0.6 N(-3, 2^2) + 0.4 N(7, 3^2)$.
% The best fitting single Normal distribution to this would be $N(1, \sqrt{30}^2)$.
%
As an
% another
example of this algorithm, consider a first moment of $0$, a $2^{\rm nd}$ moment of $6$ and a $4^{\rm th}$ moment of $3 \times 37.2 = 111.6$.
Solving the simultaneous equations gives us $0.6 N(0, 2^2) +  0.4 N(0, 3^2)$.

\subsection{The k-means++ algorithm}

The k-means++ algorithm extends the k-means method with an efficient way of choosing centres. Let $D(x)$ denote the shortest distance from a data point $x$ to the closest centre that has already been selected. The k-means++ has the following steps:

\begin{enumerate}
    \item Selects a centre $c_1$ uniformly at random from the $N$ data points.
    \item Selects a new centre $c_i$ from the data points with probability $\frac{D(x)^2}{\sum_{x\in X}}D(x)^2$
    \item Repeat Step 2. until k centres have been selected.
    \item Proceed as with the standard k-means algorithm.
\end{enumerate}

k-means++ is $O(\log k)$-Competitive.

\section{Experimental Design}

To evaluate the proposed search algorithms, we conduct a set of experiments on synthetic and real datasets. As the output of the algorithms depends on the starting seed, and some of the methods can be affected by randomness, we perform 50 runs for each search scheme and dataset. 

The different search schemes are validated using the Kolmogorov-Smirnov (KS) non-parametric test~\citep{Pettitt77S}, which checks for a statistical significance in the difference between the performances of the algorithms. The 50 runs are submitted to the KS analysis with a null hypothesis of no difference between the performances of the different search methods.  


\subsection{Benchmark Method}
We use the search algorithm proposed by Kasarapu and Allison as a benchmark for comparison, which
has been shown to be the state-of-the art
% in what sense has this been shown?
~\cite{kasarapu2015minimum}. 
\begin{algorithm}[H]
\caption{The benchmark method~\cite{kasarapu2015minimum}.}
\label{alg:kasarapu}
\begin{algorithmic}[1]
\State {\bf \sc input:} $K$ \Comment{Number of components.}
\State current $\leftarrow$ one component mixture
\While{TRUE}
\State components $\leftarrow$ current
  \For{$i\in K$}\label{line:splitBegin}\Comment{Exhaustively split all components}
    \State splits[i]$\leftarrow$ \Call{Split}{current,components[i]}
  \EndFor
\State  bestSplit$\leftarrow$\Call{Best}{splits}\label{line:splitEnd} \Comment{Record the best split.}
\If{K>1}
  \For{$i\in K$}\label{line:deleteBegin}\Comment{Exhaustively delete all components}
    \State deletes[i]$\leftarrow$ \Call{Delete}{current,components[i]}
  \EndFor
\State  bestDelete$\leftarrow$\Call{Best}{deletes} \label{line:deleteEnd}\Comment{Record the best delete.}
\EndIf
  \For{$i\in K$}\label{line:MergeBegin}\Comment{Exhaustively merge all components}
    \State j$\leftarrow$ \Call{closestComponent}{i}
    \State merges[i]$\leftarrow$ \Call{Merge}{merges}
  \EndFor
\State  bestMerge$\leftarrow$\Call{Best}{merges} \label{line:MergeEnd}\Comment{Record the best merge.}
\State bestPerturbation $\leftarrow$ \Call{Best}{bestSplit, bestDelete,bestMerge} \Comment{Select the best perturbation.}
\State $\Delta I \leftarrow$ \Call{MessageLength}{bestPerturbation}-\Call{MessageLength}{current} \Comment{Check for improvement.}
\If{$\Delta I < 0$}
\State current $\leftarrow$ bestPerturbation
\State {\bf \sc Continue}
\EndIf
\State {\bf \sc Break} \label{line:end}
\EndWhile
\State {\bf \sc output:} current
\end{algorithmic}
\end{algorithm}

The method is presented in Algorithm~\ref{alg:kasarapu}. It starts with one component and iteratively splits (lines \ref{line:splitBegin}-\ref{line:splitEnd}), deletes (lines \ref{line:deleteBegin}-\ref{line:deleteEnd}), and merges (lines \ref{line:MergeBegin}-\ref{line:MergeEnd}) components until no improvements in the message length is observed (line~\ref{line:end}). 

\subsection{Dataset}

\alda{Andy, can you please include the details?}
\paragraph{Simulations}

\paragraph{GA data}

\subsection{Criteria for Comparison}

We employ two criteria for comparing the proposed search methods to the benchmark: the message length and the Kullback Leibler divergence~\cite{kullback1951information}.

\paragraph{Message Length}

\paragraph{Kullback Leibler divergence}

\section{Results}
\section{References}
\bibliographystyle{elsarticle-num}
\bibliography{mmlbibliography}

\end{document}
